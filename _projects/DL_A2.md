---
layout: page
title: Image Captioning with Model Benchmarking
description: "CS60010: Deep Learning"
# img: assets\img\perp.png
importance: 1
category: coursework
related_publications: false
github: https://github.com/praneshvijay/DL-Spr25/tree/main/A2
---

<div class="row">
  <div class="col-sm-6 mt-3 mt-md-0 mx-auto">
    {% include figure.liquid loading="eager" path="assets/img/model_architecture.png" title="Complete Model Architecture" class="img-fluid rounded z-depth-1" %}
  </div>
</div>
<div class="caption">
  The proposed image captioning model integrates a Vision Transformer (ViT) for visual feature extraction with a GPT-2 decoder enhanced with cross-attention for text generation. ViT encodes images as patch sequences, which the GPT-2 decoder attends to while generating captions.
</div>

This coursework project presents a complete framework for **automatic image captioning**, combining state-of-the-art vision and language models in a unified architecture. The system leverages **Vision Transformers (ViT)** for image encoding and **GPT-2** for natural language generation, resulting in a transformer-based encoder-decoder model capable of producing meaningful and coherent image descriptions.

Beyond caption generation, the project includes an extensive evaluation pipeline that benchmarks the model’s performance against **SmolVLM** in a zero-shot setting and assesses **robustness under structured image occlusion**, providing a comprehensive understanding of the model’s capabilities and limitations.

---

<div class="row">
  <div class="col-sm mt-3 mt-md-0">
    {% include figure.liquid loading="eager" path="assets/img/occlusion_10.png" title="10% Occlusion" class="img-fluid rounded z-depth-1" %}
  </div>
  <div class="col-sm mt-3 mt-md-0">
    {% include figure.liquid loading="eager" path="assets/img/occlusion_50.png" title="50% Occlusion" class="img-fluid rounded z-depth-1" %}
  </div>
  <div class="col-sm mt-3 mt-md-0">
    {% include figure.liquid loading="eager" path="assets/img/occlusion_80.png" title="80% Occlusion" class="img-fluid rounded z-depth-1" %}
  </div>
</div>
<div class="caption">
  Evaluation of model robustness under structured patch-wise occlusion at 10%, 50%, and 80% levels. This setup simulates partial visual corruption common in real-world scenarios.
</div>

## Contributions

This project introduces several meaningful contributions within the scope of course-based research:

* **Cross-modal integration:** The custom model connects ViT and GPT-2 using a tailored cross-attention mechanism, enabling effective fusion of visual and linguistic information.
* **Patch-wise occlusion analysis:** Images are divided into 16×16 patches, with random masking applied to simulate partial corruption. This controlled occlusion provides a systematic way to evaluate model robustness.
* **Training methodology:** A staged unfreezing strategy is adopted to gradually fine-tune the pretrained components, enhancing convergence and stability.

---

<div class="row justify-content-sm-center">
  <div class="col-sm mt-3 mt-md-0">
    {% include figure.liquid path="assets/img/perp.png" title="Training and Validation Perplexity over Epochs" class="img-fluid rounded z-depth-1" %}
  </div>
  <div class="col-sm mt-3 mt-md-0">
    {% include figure.liquid path="assets/img/loss.png" title="Training and Validation Loss over Epochs" class="img-fluid rounded z-depth-1" %}
  </div>
</div>
<div class="caption">
  Perplexity and loss curves show steady improvements, reflecting successful model adaptation through staged unfreezing.
</div>

## Experimental Evaluation

The model’s performance is evaluated using standard metrics:

* **BLEU:** 0.0493
* **ROUGE-L:** 0.2216
* **METEOR:** 0.2450

Although the scores are modest, they reflect the model’s ability to learn from limited data and provide a strong foundation for further improvements. Notably, the custom model demonstrates **greater resilience under occlusion** compared to the zero-shot SmolVLM baseline. At 80% occlusion, it maintains relatively stable captioning quality, highlighting the benefits of task-specific fine-tuning.

In addition, a **BERT-based classifier** was trained to distinguish between captions generated by different models. It achieved:

* **Precision:** 99.72%
* **Recall:** 99.44%
* **F1-score:** 99.58%

These results suggest that captioning models produce linguistically distinguishable outputs, which may be exploited for model auditing and evaluation.

---

<div class="row">
  <div class="col-sm-6 mt-3 mt-md-0 mx-auto">
    {% include figure.liquid loading="eager" path="assets/img/bert.png" title="Classification Performance" class="img-fluid rounded z-depth-1" %}
  </div>
</div>
<div class="caption">
  BERT-based classifier training shows rapid convergence and high accuracy, supporting its use for caption source identification.
</div>

## Future Work

This coursework has provided valuable experience in building and evaluating multimodal deep learning models. The robustness analysis, in particular, emphasized the importance of **real-world reliability** in captioning systems where visual input may be incomplete or corrupted.

The strong performance of the BERT classifier points to potential applications in **caption quality monitoring** or **model selection** in production workflows. Future extensions may involve:

* Using larger datasets and stronger decoders (e.g., GPT-Neo or T5).
* Incorporating multimodal pretraining strategies.
* Exploring interpretability techniques like attention visualization to better understand model decisions.

---

<div class="row mb-4">
    <div class="col-12 text-center">
        <a href="/assets/pdf/team_id_35_report.pdf"
            class="btn me-3 text-white"
            style="background-color: #00ab37; border-color: #00ab37;"
            target="_blank">
            <i class="fas fa-file-pdf"></i> Read Full Report
        </a>
    </div>
</div>
